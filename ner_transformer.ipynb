{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb - layers.Embedding(\n",
    "            input_dim=maxlen, output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\dce97da350908f6c978cf452149c3683e4f5c5ab7005b3ae5d3c986a93332525...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12592/199143676.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mconll_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"conll2003.py\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\load.py\u001B[0m in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    855\u001B[0m     \u001B[1;31m# Download and prepare data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 856\u001B[1;33m     builder_instance.download_and_prepare(\n\u001B[0m\u001B[0;32m    857\u001B[0m         \u001B[0mdownload_config\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdownload_config\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    858\u001B[0m         \u001B[0mdownload_mode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdownload_mode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\builder.py\u001B[0m in \u001B[0;36mdownload_and_prepare\u001B[1;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001B[0m\n\u001B[0;32m    581\u001B[0m                             \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"HF google storage unreachable. Downloading and preparing it from source\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    582\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mdownloaded_from_gcs\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 583\u001B[1;33m                         self._download_and_prepare(\n\u001B[0m\u001B[0;32m    584\u001B[0m                             \u001B[0mdl_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdl_manager\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverify_infos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverify_infos\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mdownload_and_prepare_kwargs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m                         )\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\builder.py\u001B[0m in \u001B[0;36m_download_and_prepare\u001B[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001B[0m\n\u001B[0;32m    637\u001B[0m         \u001B[0msplit_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSplitDict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    638\u001B[0m         \u001B[0msplit_generators_kwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_split_generators_kwargs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprepare_split_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 639\u001B[1;33m         \u001B[0msplit_generators\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_split_generators\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdl_manager\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0msplit_generators_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    640\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    641\u001B[0m         \u001B[1;31m# Checksums verification\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\conll2003\\dce97da350908f6c978cf452149c3683e4f5c5ab7005b3ae5d3c986a93332525\\conll2003.py\u001B[0m in \u001B[0;36m_split_generators\u001B[1;34m(self, dl_manager)\u001B[0m\n\u001B[0;32m    194\u001B[0m             \u001B[1;34m\"test\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34mf\"{_URL}{_TEST_FILE}\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    195\u001B[0m         }\n\u001B[1;32m--> 196\u001B[1;33m         \u001B[0mdownloaded_files\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdl_manager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdownload_and_extract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murls_to_download\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    197\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    198\u001B[0m         return [\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\download_manager.py\u001B[0m in \u001B[0;36mdownload_and_extract\u001B[1;34m(self, url_or_urls)\u001B[0m\n\u001B[0;32m    287\u001B[0m             \u001B[0mextracted_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mextracted\u001B[0m \u001B[0mpaths\u001B[0m \u001B[0mof\u001B[0m \u001B[0mgiven\u001B[0m \u001B[0mURL\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    288\u001B[0m         \"\"\"\n\u001B[1;32m--> 289\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextract\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl_or_urls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    290\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget_recorded_sizes_checksums\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\download_manager.py\u001B[0m in \u001B[0;36mdownload\u001B[1;34m(self, url_or_urls)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m         \u001B[0mstart_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 197\u001B[1;33m         downloaded_path_or_paths = map_nested(\n\u001B[0m\u001B[0;32m    198\u001B[0m             \u001B[0mdownload_func\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    199\u001B[0m             \u001B[0murl_or_urls\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\py_utils.py\u001B[0m in \u001B[0;36mmap_nested\u001B[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\u001B[0m\n\u001B[0;32m    201\u001B[0m         \u001B[0mnum_proc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    202\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mnum_proc\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterable\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[0mnum_proc\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 203\u001B[1;33m         mapped = [\n\u001B[0m\u001B[0;32m    204\u001B[0m             \u001B[0m_single_map_nested\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtypes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdisable_tqdm\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    205\u001B[0m         ]\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\py_utils.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    202\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mnum_proc\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterable\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[0mnum_proc\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m         mapped = [\n\u001B[1;32m--> 204\u001B[1;33m             \u001B[0m_single_map_nested\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtypes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdisable_tqdm\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    205\u001B[0m         ]\n\u001B[0;32m    206\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\py_utils.py\u001B[0m in \u001B[0;36m_single_map_nested\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m    140\u001B[0m     \u001B[1;31m# Singleton first to spare some computation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    141\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_struct\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_struct\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtypes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 142\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mfunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_struct\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    144\u001B[0m     \u001B[1;31m# Reduce logging to keep things readable in multiprocessing with tqdm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\download_manager.py\u001B[0m in \u001B[0;36m_download\u001B[1;34m(self, url_or_filename, download_config)\u001B[0m\n\u001B[0;32m    218\u001B[0m             \u001B[1;31m# append the relative path to the base_path\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    219\u001B[0m             \u001B[0murl_or_filename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0murl_or_path_join\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_base_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl_or_filename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 220\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mcached_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl_or_filename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdownload_config\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdownload_config\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    221\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0miter_archive\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\file_utils.py\u001B[0m in \u001B[0;36mcached_path\u001B[1;34m(url_or_filename, download_config, **download_kwargs)\u001B[0m\n\u001B[0;32m    279\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mis_remote_url\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl_or_filename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    280\u001B[0m         \u001B[1;31m# URL, so get it from the cache (downloading if necessary)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 281\u001B[1;33m         output_path = get_from_cache(\n\u001B[0m\u001B[0;32m    282\u001B[0m             \u001B[0murl_or_filename\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    283\u001B[0m             \u001B[0mcache_dir\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcache_dir\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\software\\anaconda3\\envs\\tf-nightly\\lib\\site-packages\\datasets\\utils\\file_utils.py\u001B[0m in \u001B[0;36mget_from_cache\u001B[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\u001B[0m\n\u001B[0;32m    632\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Couldn't find file at {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    633\u001B[0m         \u001B[0m_raise_if_offline_mode_is_enabled\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Tried to reach {url}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 634\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mConnectionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Couldn't reach {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    635\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    636\u001B[0m     \u001B[1;31m# Try a second time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mConnectionError\u001B[0m: Couldn't reach https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt"
     ]
    }
   ],
   "source": [
    "conll_data = load_dataset(\"conll2003.py\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            tokens = record[\"tokens\"]\n",
    "            f.write(\n",
    "                str(len(tokens))\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(tokens)\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(map(str, ner_tags))\n",
    "                + \"\\n\"\n",
    "            )\n",
    "data_dir = \"datas/conll2003\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "export_to_file(os.path.join(data_dir,\"conll_train.txt\"), conll_data[\"train\"])\n",
    "export_to_file(os.path.join(data_dir,\"conll_val.txt\"), conll_data[\"validation\"])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B_PER', 3: 'I_PER', 4: 'B_ORG', 5: 'I_ORG', 6: 'B_LOC', 7: 'I_LOC', 8: 'B_MISC', 9: 'I_MISC'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"_\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels)+1), all_labels))\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "counter = Counter(all_tokens_array)\n",
    "\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "lookup_layer = layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=vocabulary\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = tf.data.TextLineDataset(os.path.join(data_dir, \"conll_train.txt\"))\n",
    "val_data = tf.data.TextLineDataset(os.path.join(data_dir, \"conll_val.txt\"))\n",
    "\n",
    "print(list(train_data.take(1).as_numpy_iterator()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1: length+1]\n",
    "    tags = record[length+1:]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return token, tags\n",
    "\n",
    "\n",
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super(CustomNonPaddingTokenLoss, self).__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=10)\n",
    "\n",
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)\n",
    "\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"eu rejects german call to boycott british lamb\"\n",
    ")\n",
    "\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "print(prediction)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "    evaluate(real_tags, predicted_tags)\n",
    "\n",
    "calculate_metrics(val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}